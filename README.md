# ğŸ§  Reinforcement Learning Framework | å¼ºåŒ–å­¦ä¹ æ¡†æ¶

A structured, bilingual collection of reinforcement learning algorithms implemented from scratch, organized by methodology.

---

## 1ï¸âƒ£ Value-based Methods | åŸºç¡€ä»·å€¼è¿­ä»£ç±»

**Dynamic Programming (DP) | åŠ¨æ€è§„åˆ’**
- Policy Evaluation â˜‘ï¸
- Policy Iteration â˜‘ï¸
- Value Iteration â˜‘ï¸  

**Monte Carlo Methods | è’™ç‰¹å¡ç½—æ–¹æ³•**

**Temporal Difference (TD) | æ—¶åºå·®åˆ†æ³•**
- TD(0) â˜‘ï¸  
- TD(n) â˜‘ï¸
- TD(Î») â˜‘ï¸  

**Q-learning Family | Q-learning ç³»åˆ—**
- Q-learning (off-policy) â˜‘ï¸ 
- SARSA (on-policy) â˜‘ï¸  
- Expected SARSA â˜‘ï¸ 
- Double Q-learning â˜‘ï¸ 

**Deep Value Networks | æ·±åº¦ä»·å€¼ç½‘ç»œ**
- DQN â˜‘ï¸ 
- Double DQN  
- Dueling DQN  
- Rainbow DQN  

---

## 2ï¸âƒ£ Policy-based Methods | ç­–ç•¥æ¢¯åº¦ç±»

- **REINFORCE (Vanilla Policy Gradient) | çº¯ç­–ç•¥æ¢¯åº¦**  
- **Baseline Methods | åŸºçº¿æ³•**  
- **Actor-Critic Methods | Actor-Critic**
  - A2C / A3C  
  - DDPG (Continuous Action Space | è¿ç»­åŠ¨ä½œç©ºé—´)  
  - TD3  
  - SAC (Soft Actor-Critic)

---

## 3ï¸âƒ£ Model-based Methods | æ¨¡å‹é©±åŠ¨ç±»

- Dyna-Q  
- World Models  
- MBPO (Model-Based Policy Optimization)

---

## 4ï¸âƒ£ Advanced / Modern RL | é«˜çº§ä¼˜åŒ–ç±»

- PPO (Proximal Policy Optimization)  
- TRPO (Trust Region Policy Optimization)  
- GAE (Generalized Advantage Estimation)  
- DPO / GRPO (Reward Model Alignment)

---

## 5ï¸âƒ£ Multi-Agent & Hierarchical RL | å¤šæ™ºèƒ½ä½“ / å±‚çº§ç±»

- Multi-Agent Q-learning  
- MADDPG  
- HRL (Options Framework)

---

## 6ï¸âƒ£ Application Domains | åº”ç”¨åœºæ™¯ç±»

- ğŸ® **Game Environments**: CartPole / LunarLander / Mario / Atari  
  ï¼ˆæ¸¸æˆç¯å¢ƒï¼‰  
- ğŸ§© **Recommender Systems / Advertising Bidding Optimization**  
  ï¼ˆæ¨èç³»ç»Ÿ / å¹¿å‘Šå‡ºä»·ä¼˜åŒ–ï¼‰  
- ğŸ¤– **Robotics / Control** ï¼ˆæœºå™¨äºº / æ§åˆ¶ï¼‰  
- ğŸ’° **Finance / Trading Strategies** ï¼ˆé‡‘è / äº¤æ˜“ç­–ç•¥ï¼‰  
- âœï¸ **Text Generation / LLM Fine-Tuning (RLHF / PPO)** ï¼ˆæ–‡æœ¬ç”Ÿæˆ / LLM è°ƒä¼˜ï¼‰

---

## ğŸ“˜ Notes

- All implementations follow the same environment and logging conventions.  
- Each subfolder contains an independent example (`.py` file) with documentation and visualization.  
- This repository aims to serve as a clean educational reference for RL learners and practitioners.

---

â­ **Author:** Althea Lam  
ğŸ“š **Language:** Python 3.9+  
ğŸ—ï¸ **Frameworks:** NumPy Â· Gym Â· PyTorch  
ğŸ“„ **License:** MIT  
