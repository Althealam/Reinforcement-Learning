{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class InventoryEnv:\n",
    "    def __init__(self, capacity=40, demand_dist='uniform'):\n",
    "        assert demand_dist in ['uniform', 'poisson']\n",
    "        self.capacity = capacity\n",
    "        self.gamma = 0.25\n",
    "\n",
    "        # 成本设置\n",
    "        self.variable_cost = 1 \n",
    "        self.fixed_cost = 200 # 固定成本\n",
    "        self.holding_cost = 1 # 存活成本\n",
    "        self.selling_price = 10 # 销售价格\n",
    "\n",
    "        if demand_dist=='uniform':\n",
    "            self.max_demand = capacity # 最大需求量不可以超过存货的最大值\n",
    "            # demand只会出现在0, 1, ..., 40这几个值，因此是41个值\n",
    "            self.demand_lst = torch.arange(self.max_demand+1) # shape = (capacity+1, )\n",
    "            self.demand_probs = torch.ones(self.max_demand+1)/(self.max_demand+1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"后面再写poisson\")\n",
    "    \n",
    "    def get_state_space(self):\n",
    "        \"\"\"\n",
    "        state = 当前库存数量\n",
    "        如果仓库容量是capacity，那么库存不可能小于0，并且不可能大于capacity\n",
    "        \"\"\"\n",
    "        return torch.arange(self.capacity+1)\n",
    "    \n",
    "    def get_action_space(self):\n",
    "        \"\"\"\n",
    "        action = 订货数量(order quantity)\n",
    "        可以订货0个，也可以订货1个、2个，一直到capacity个\n",
    "        \"\"\"\n",
    "        return torch.arange(self.capacity+1)\n",
    "    \n",
    "    def one_step_cost(self, state, action, demand):\n",
    "        \"\"\"\n",
    "        state: 当前库存 \n",
    "        action: 订货量\n",
    "        demand: 需求量\n",
    "        \"\"\"\n",
    "        # 订货成本：如果进货a，那么固定成本为fixed_cost*I(a>0)+c*a\n",
    "        acquisition = self.fixed_cost * (action>0) + self.variable_cost * action\n",
    "        # 库存成本：如果demand<(state+action)，说明有剩余\n",
    "        holding = self.holding_cost*torch.clamp(state+action-demand, min=0)\n",
    "        # 缺货成本：p*max(demand-state-action, 0)\n",
    "        lost_sales = self.selling_price * torch.clamp(demand-state-action, min=0)\n",
    "        return acquisition+holding+lost_sales\n",
    "    \n",
    "    def simulation(self, policy, num_episodes=100, days_per_episode=100):\n",
    "        \"\"\"\n",
    "        用策略policy进行模拟\n",
    "        return:\n",
    "        : sim_states: (L, T+1)\n",
    "        : sim_actions: (L, T)\n",
    "        : sim_demands: (L, T)\n",
    "        : sim_costs: (L, T)\n",
    "        \"\"\"\n",
    "        L = num_episodes # 表示轨迹的数量\n",
    "        T = days_per_episode # 表示每个轨迹有多少天\n",
    "        # s0, a0, r0, s1, a1, r1, s2, a2, r2 # 轨迹1，并且有三天\n",
    "\n",
    "        # 初始化结果，表示经过模拟后的states, actions, demands, costs\n",
    "        # 分别有L个轨迹，每个轨迹有T天\n",
    "        sim_states = torch.zeros((L, T+1))\n",
    "        sim_actions = torch.zeros((L, T))\n",
    "        sim_demands = torch.zeros((L, T))\n",
    "        sim_costs = torch.zeros((L, T))\n",
    "\n",
    "        # 随机初始库存\n",
    "        sim_states[:, 0] = torch.randint(low=0, high=self.capacity+1, size = (L, ))\n",
    "\n",
    "        for t in range(T):\n",
    "            # 当前episode的库存state\n",
    "            states = sim_states[:, t] # 当前episode的第t天的策略\n",
    "            print(states)\n",
    "\n",
    "            # 根据策略选择动作（动作概率分布）\n",
    "            with torch.no_grad():\n",
    "                # policy输入shape必须是(L, 1)\n",
    "                action_probs = policy(states.view(-1, 1))\n",
    "\n",
    "                # 构造可行动作mask（不能超过capacity-state)\n",
    "                action_idx = torch.arange(self.capacity+1).unsqueeze(0) # shape = (1, capacity+1)\n",
    "                limits = (self.capacity-states.long()).unsqueeze(1) # shape = (L, 1)\n",
    "                # unsqueeze(0)可以让维度1的位置加入一个1\n",
    "                mask = (action_idx<=limits).float()\n",
    "\n",
    "                # 屏蔽非法动作\n",
    "                masked_probs = action_probs * mask\n",
    "\n",
    "                # 防止除以0，重新归一化\n",
    "                masked_probs = masked_probs/masked_probs.sum(dim=1, keepdim=True)\n",
    "\n",
    "            # 采样动作\n",
    "            actions = torch.multinomial(masked_probs, num_samples=1).squeeze(1) # shape = (L, )\n",
    "\n",
    "            # 随机生成需求\n",
    "            demands = torch.multinomial(self.demand_probs, num_samples = L, replacement=True)\n",
    "\n",
    "            # 计算成本\n",
    "            costs = self.one_step_cost(states, actions, demands)\n",
    "\n",
    "            # 更新下一个state\n",
    "            next_states = torch.clamp(states+actions-demands, 0, self.capacity)\n",
    "\n",
    "            # 写入结果\n",
    "            sim_states[:, t+1] = next_states\n",
    "            sim_actions[:, t] = actions\n",
    "            sim_demands[:, t] = actions\n",
    "            sim_costs[:, t] = costs\n",
    "        return sim_states, sim_actions, sim_demands, sim_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1063f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input states shape =  torch.Size([5, 1])\n",
      "output values shape =  torch.Size([5, 1])\n",
      "values =  tensor([[-0.1468],\n",
      "        [-0.2407],\n",
      "        [-0.2691],\n",
      "        [-0.2735],\n",
      "        [-0.2740]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 128):\n",
    "        \"\"\"\n",
    "        V(s): 输入state，比如库存数，输出一个标量value\n",
    "        input_dim: 状态维度，这里是1，只有库存一个数\n",
    "        hidden_dim: 隐藏层维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape = (batch_size, input_dim)\n",
    "        return: shape = (batch_size, 1)\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "states = torch.tensor([[0.0],[10],[20],[30],[40]]) # shape = (5, 1) \n",
    "value_net = ValueNetwork(input_dim=1, hidden_dim=128)\n",
    "values = value_net(states)\n",
    "print(\"input states shape = \", states.shape)\n",
    "print(\"output values shape = \", values.shape)\n",
    "print(\"values = \", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1aa854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs.shape: torch.Size([5, 41])\n",
      "每行概率之和: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = 128):\n",
    "        \"\"\"\n",
    "        input: state (batch_size x input_dim)\n",
    "        output: the probabilties of actions (batch_size x output_dim)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "policy_network = PolicyNetwork(input_dim=1, output_dim=41)\n",
    "states = torch.tensor([[0.0],[10.0],[20.0],[30.0],[40.0]])\n",
    "probs = policy_network(states)\n",
    "print(\"probs.shape:\", probs.shape)\n",
    "print(\"每行概率之和:\", probs.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2d0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demand_lst: torch.Size([41])\n",
      "demand_probs: torch.Size([41])\n",
      "sum of probs: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnv(capacity = 40)\n",
    "print(\"demand_lst:\", env.demand_lst.shape)\n",
    "print(\"demand_probs:\", env.demand_probs.shape)\n",
    "print(\"sum of probs:\", env.demand_probs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40])\n"
     ]
    }
   ],
   "source": [
    "print(env.get_action_space()) # 动作空间的值，表示可以订购的数量\n",
    "print(env.get_state_space()) # 状态空间的值，表示当前的库存量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad92ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([208, 206, 224,   0])\n"
     ]
    }
   ],
   "source": [
    "states = torch.tensor([10, 20, 5, 0])\n",
    "actions = torch.tensor([5, 3, 10, 0])\n",
    "demands = torch.tensor([12, 20, 1, 0])\n",
    "\n",
    "print(env.one_step_cost(states, actions, demands))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23., 20., 26.])\n",
      "tensor([ 0., 19., 13.])\n",
      "tensor([19.,  0.,  0.])\n",
      "tensor([18.,  5.,  0.])\n",
      "tensor([ 3.,  0., 11.])\n",
      "states.shape  = torch.Size([3, 6])\n",
      "actions.shape = torch.Size([3, 5])\n",
      "demands.shape = torch.Size([3, 5])\n",
      "costs.shape   = torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnv(capacity=40)\n",
    "\n",
    "# 假 policy：每个动作概率都一样\n",
    "policy_dummy = lambda x: torch.ones((len(x), env.capacity+1)) / (env.capacity+1)\n",
    "\n",
    "# 总共有3个episode，每个episode有5天，现在需要通过这三个episode来进行模拟\n",
    "# 生成state，action，demand，cost\n",
    "states, actions, demands, costs = env.simulation(policy_dummy, num_episodes=3, days_per_episode=5)\n",
    "\n",
    "print(\"states.shape  =\", states.shape)    # 期望 (3, 6)\n",
    "print(\"actions.shape =\", actions.shape)   # 期望 (3, 5)\n",
    "print(\"demands.shape =\", demands.shape)   # 期望 (3, 5)\n",
    "print(\"costs.shape   =\", costs.shape)     # 期望 (3, 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prod_env)",
   "language": "python",
   "name": "prod_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
